import os
import json
import logging
from pathlib import Path
from funasr import AutoModel

class SpeechRecognizer:
    def __init__(self):
        """
        åˆå§‹åŒ– ASR æ¨¡å‹ã€‚
        """
        self.config = self._load_config()
        self._prepare_modelscope_env()
        print("æ­£åœ¨åŠ è½½ FunASR æ¨¡å‹...")
        self.model = AutoModel(
            model="iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
            vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
            punc_model="iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            trust_remote_code=False,
            disable_update=True,
        )
        print("âœ… FunASR æ¨¡å‹åŠ è½½å®Œæˆ")

    def _load_config(self) -> dict:
        config_path = Path("config.json")
        if not config_path.exists():
            return {}
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    def _prepare_modelscope_env(self):
        funasr_cfg = self.config.get("funasr", {})
        offline_when_cached = funasr_cfg.get("offline_when_cached", True)
        quiet = funasr_cfg.get("quiet", True)

        if quiet:
            os.environ.setdefault("MODELSCOPE_LOG_LEVEL", "ERROR")
            logging.getLogger().setLevel(logging.ERROR)
            logging.getLogger("modelscope").setLevel(logging.ERROR)
            logging.getLogger("modelscope.utils").setLevel(logging.ERROR)

        if offline_when_cached:
            cache_root = Path.home() / ".cache" / "modelscope" / "hub" / "models" / "iic"
            model_dirs = [
                "speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
                "speech_fsmn_vad_zh-cn-16k-common-pytorch",
                "punc_ct-transformer_zh-cn-common-vocab272727-pytorch",
            ]
            all_cached = all((cache_root / name).exists() for name in model_dirs)
            if all_cached:
                os.environ["MODELSCOPE_OFFLINE"] = "1"

    def _extract_audio_if_needed(self, video_path: Path) -> Path:
        """
        å†…éƒ¨é€»è¾‘ï¼šåˆ¤æ–­å¹¶æå–éŸ³é¢‘ã€‚
        è¿”å›ç”Ÿæˆçš„ï¼ˆæˆ–å·²å­˜åœ¨çš„ï¼‰éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
        """
        audio_path = video_path.with_suffix(".wav")

        # 1. å¦‚æœéŸ³é¢‘å·²ç»å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if audio_path.exists():
            # print(f"ğŸ” å‘ç°å·²æœ‰éŸ³é¢‘ï¼Œè·³è¿‡æå–: {audio_path.name}")
            return audio_path

        # 2. å¦‚æœä¸å­˜åœ¨ï¼Œè°ƒç”¨ ffmpeg æå–
        print(f"ğŸ”¨ æ­£åœ¨æå–éŸ³é¢‘: {video_path.name} -> {audio_path.name}")
        
        # è¿™é‡Œçš„ ffmpeg å‘½ä»¤ï¼š
        # -vn: å»æ‰è§†é¢‘æµ
        # -ac 1: å•å£°é“ (ASR åªéœ€è¦å•å£°é“)
        # -ar 16000: 16k é‡‡æ ·ç‡ (æ¨¡å‹è¦æ±‚)
        cmd = f'ffmpeg -i "{video_path}" -ac 1 -ar 16000 -vn "{audio_path}" -y -v quiet'
        exit_code = os.system(cmd)
        
        if exit_code != 0:
            raise RuntimeError(f"âŒ ffmpeg æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥è§†é¢‘è·¯å¾„æˆ–æ˜¯å¦å®‰è£…äº† ffmpegã€‚")
            
        return audio_path

    def transcribe(self, video_file: str, output_srt: bool = False):
        """
        ä¸»å…¥å£ï¼šä¼ å…¥è§†é¢‘è·¯å¾„ï¼Œè¿”å›è¯†åˆ«ç»“æœåˆ—è¡¨ã€‚
        è‡ªåŠ¨å¤„ç†éŸ³é¢‘æå–é€»è¾‘ã€‚
        
        å‚æ•°ï¼š
            video_file: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_srt: å¯é€‰ï¼Œæ˜¯å¦ç”Ÿæˆ SRT å­—å¹•æ–‡ä»¶ã€‚
                       å¦‚æœä¸º Trueï¼Œåˆ™åœ¨é¡¹ç›®æ ¹ç›®å½•ç”Ÿæˆä¸è§†é¢‘æ–‡ä»¶åŒåçš„ .srt æ–‡ä»¶ã€‚
                       é»˜è®¤ä¸º Falseã€‚
        """
        video_path = Path(video_file)
        
        if not video_path.exists():
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶ {video_path}")
            return []

        # Step 1: è‡ªåŠ¨è·å–/ç”ŸæˆéŸ³é¢‘
        try:
            audio_path = self._extract_audio_if_needed(video_path)
        except Exception as e:
            print(e)
            return []

        # Step 2: æ‰§è¡Œè¯†åˆ«
        print(f"ğŸ™ï¸ æ­£åœ¨è¯†åˆ«: {video_path.name} ...")
        
        # FunASR æ¨ç†
        res = self.model.generate(
            input=str(audio_path),            
            batch_size_s=300,            
            sentence_timestamp=True,      
            return_timestamps=True,       
            disable_pbar=False,
            trust_remote_code=False            
        )

        segments = []
        if res and len(res) > 0:
            # ä¼˜å…ˆè¯»å– VAD åˆ‡åˆ†åçš„ sentence_info
            if "sentence_info" in res[0]:
                for item in res[0]["sentence_info"]:
                    text_clean = item["text"].replace(" ", "")
                    segments.append({
                        "text": text_clean,
                        "start": item["start"] / 1000.0, # æ¯«ç§’è½¬ç§’
                        "end": item["end"] / 1000.0,
                    })
            # å…œåº•ï¼šå¦‚æœæ²¡æœ‰åˆ‡åˆ†ä¿¡æ¯ï¼ˆæçŸ­è¯­éŸ³ï¼‰
            elif "text" in res[0]:
                 segments.append({
                    "text": res[0]["text"].replace(" ", ""),
                    "start": 0.0,
                    "end": 0.0,
                })
        
        # Step 3: å¦‚æœéœ€è¦ç”Ÿæˆ SRT æ–‡ä»¶
        if output_srt:
            # æ ¹æ®è§†é¢‘æ–‡ä»¶åç”Ÿæˆå¯¹åº”çš„ SRT æ–‡ä»¶åï¼Œä¿å­˜åœ¨è§†é¢‘æ‰€åœ¨ç›®å½•
            srt_filename = video_path.stem + ".srt"
            srt_path = video_path.parent / srt_filename
            self.generate_srt(segments, str(srt_path))
        
        return segments

    def generate_srt(self, segments, output_srt_path: str):
        """
        è¾…åŠ©å·¥å…·ï¼šç”Ÿæˆ SRT æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        """
        def _fmt(seconds):
            m, s = divmod(seconds, 60)
            h, m = divmod(m, 60)
            return f"{int(h):02d}:{int(m):02d}:{int(s):02d},{int(s*1000)%1000:03d}"

        with open(output_srt_path, "w", encoding="utf-8") as f:
            for i, seg in enumerate(segments):
                start = _fmt(seg['start'])
                end = _fmt(seg['end'])
                f.write(f"{i+1}\n{start} --> {end}\n{seg['text']}\n\n")
        print(f"ğŸ“„ SRT å­—å¹•å·²ç”Ÿæˆ: {output_srt_path}")

# ==========================================
# è°ƒç”¨ç¤ºä¾‹
# ==========================================
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–
    recognizer = SpeechRecognizer()

    # 2. ç›´æ¥ä¸¢è§†é¢‘è·¯å¾„è¿›å» (ä¸ç®¡æœ‰æ²¡æœ‰éŸ³é¢‘ï¼Œå®ƒè‡ªå·±ä¼šå¤„ç†)
    video_file = "äº‘å—çš„æ—©é¤å¤šç€å‘¢~ç±³æµ†ç²‘ç²‘çœ‹æ‹›ï¼.mp4"
    
    # 3. è·å–ç»“æœ List[Dict]
    # æ–¹å¼ Aï¼šä¸ç”Ÿæˆ SRT æ–‡ä»¶
    # results = recognizer.transcribe(video_file)
    
    # æ–¹å¼ Bï¼šç”Ÿæˆ SRT æ–‡ä»¶ï¼ˆè‡ªåŠ¨æ ¹æ®è§†é¢‘åç”Ÿæˆï¼Œä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼‰
    # ç”Ÿæˆçš„æ–‡ä»¶åä¸ºï¼šäº‘å—çš„æ—©é¤å¤šç€å‘¢~ç±³æµ†ç²‘ç²‘çœ‹æ‹›ï¼.srt
    results = recognizer.transcribe(video_file, output_srt=True)

    # 4. æ‰“å°çœ‹çœ‹
    print(f"è¯†åˆ«å®Œæˆï¼Œå…± {len(results)} å¥ã€‚")
    if len(results) > 0:
        print(f"ç¬¬ä¸€å¥: {results[0]}")